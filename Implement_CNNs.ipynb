{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of [ML-Assignment01].ipynb","version":"0.3.2","provenance":[{"file_id":"1eqlPE0E7zbzBojyrD4DdpJwBxWdI8ySs","timestamp":1542182456044}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"2mFACUKZfXRc","colab_type":"text"},"cell_type":"markdown","source":["# [ML-Assignment 1]  Convolutional Neural Networks\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","## HIỆN THỰC MẠNG CNNs - DATASET MNIST\n","\n","###1 GIỚI THIỆU CHUNG\n","\n","CNN là một trong những thuật toán DL cho kết quả tốt nhất hiện nay trong hầu hết các bài toán về thị giác máy như phân lớp, nhận dạng, … Về cơ bản CNN là một kiểu mạng ANN truyền thẳng, trong đó kiến trúc chính gồm nhiều thành phần được ghép nối với nhau theo cấu trúc nhiều tầng đó là : Convolution, Pooling, ReLU và Fully connected.\n","\n","Trong bài tập lớn này, nhóm hiện thực một mô hình mạng CNNs với các lớp như CNN, Softmax, Relu, Linear, Maxpooling, Dropout với Dataset Mnist. \n","Nội dung lần lượt trình bày các bước hiện thực mô hình, kết quả training và đánh giá mô hình với thư viện Keras.\n","![alt text](https://cdn-images-1.medium.com/max/1200/1*z7hd8FZeI_eodazwIapvAw.png)\n","\n","\n","\n"]},{"metadata":{"id":"rCbX9brih3Ok","colab_type":"text"},"cell_type":"markdown","source":["###2 NỘI DUNG HIỆN THỰC\n","####2.1 Import nguồn dữ liệu Mnist\n","\n","Tập dữ liệu bao gồm các cặp “hình chữ số viết tay” và “nhãn”. Chữ số nằm trong khoảng từ 0 đến 9, nghĩa là tổng cộng có 10 mẫu. \n","\n","Hình chữ số viết tay: Đây là ảnh màu xám với kích thước 28 x 28 pixel.\n","\n","Label: Là số thực tế mà hình chữ số viết tay này ứng với ảnh có giá trị từ 0 đến 9.\n","\n","Kết quả thu được từ thư viện Keras là 1 bộ 60000 dữ liệu train và  10000 dữ liệu test.\n","\n","\n","\n"]},{"metadata":{"id":"Wo0C2_Nz7dlh","colab_type":"text"},"cell_type":"markdown","source":["####2.2 Hiện thực Convolution Layer\n","Tầng Convolution (Conv) là tầng quan trọng nhất trong cấu trúc của CNN. Conv dựa trên lý thuyết xử lý tín hiệu số, việc lấy tích chập sẽ giúp trích xuất được những thông tin quan trọng từ dữ liệu.\n","\n","\n","####2.3 Hiện thực Max-Pooling Layer\n","\n","\n","Tầng pooling (hay còn gọi subsampling hoặc downsample) là một trong những thành phần tính toán chính trong cấu trúc CNN. Xét về mặt toán học pooling thực chất là quá trình tính toán trên ma trận trong đó mục tiêu sau khi tính toán là giảm kích thước ma trận nhưng vẫn làm nổi bật lên được đặc trưng có trong ma trận đầu vào. Trong CNN toán tử pooling được thực hiện độc lập trên mỗi kênh màu của ma trận ảnh đầu vào.\n","\n","\n","####2.4 Hiện thực Fully Connected Layer\n","Fully-connected là cách kết nối các neural ở hai tầng với nhau trong đó tầng sau kết nối đẩy đủ với các neural ở tầng trước nó. Đây cũng là dạng kết nối thường thấy ở ANN, trong CNN tầng này thường được sử dụng ở các tầng phí cuối của kiến trúc mạng.\n","\n","\n","\n","####2.5 Hiện thực Relu Layer\n","\n","Nếu tất cả các neural được tổng hợp bởi các phép biến đổi tuyến tính thì một mạng neural đều có thể đưa về dưới dạng một hàm tuyến tính. Khi đó mạng ANN sẽ đưa các bài toán về logistic regression. Do đó tại mỗi neural cần có một hàm truyền dưới dạng phi tuyến. Có nhiều dạng hàm phi tuyến được sử dụng trong quá trình này như đã giới thiệu trong phần neural căn bản. Tuy nhiên, các nghiên cứu gần đây chứng minh được việc sử dụng hàm ReLu (Rectified Linear Unit) cho kết quả tốt hơn ở các khía cạnh: \n","- Tính toán đơn giản \n","\n","- Tạo ra tính thưa (sparsity) ở các neural ẩn.\n","\n","- Quá trình huấn luyện nhanh hơn ngay cả khi không phải trải qua bước tiền huấn luyện. \n","\n","\n","\n","\n","####2.6 Hiện thực Dropout Layer\n","Drop-out là một kĩ thuật Regularization để chống lại vấn đề overfitting. Cách Drop-out thực hiện là xoá bỏ một số unit trong các step training ứng với một giá trị xác suất p cho trước.\n","\n","- Drop-Out được áp dụng trên một layer của mạng NN với một xác suất p cho trước (ta có thể sử dụng nhiều Drop-Out khác nhau cho những layer khác nhau, nhưng trên 1 layer sẽ chỉ có 1 Drop-Out). \n","\n","- Tại mỗi step trong quá trình training, khi thực hiện Forward Propagation (Lan truyền xuôi) đến layer sử dụng Drop-Out, thay vì tính toán tất cả unit có trên layer, tại mỗi unit ta “gieo xúc xắc” xem unit đó có được tính hay không dựa trên xác suất p. Với những unit được tính, ta tính toán bình thường còn với những unit không được tính giá trị tại unit đó = 0. \n","\n","- Khi thực hiện tính toán trên mạng NN trong quá trình test (sử dụng mạng NN để dự đoán) thay vì làm như trên, ta thực hiện tính toán trên tất cả các unit nhưng trọng số trên mỗi connect đến các unit của layer được áp dụng Drop-Out được thay thế bằng giá trị của trọng số đó với xác suất p hay θ := θ * p\n","\n","\n","####2.7 Learning Rate Decay\n","\n","Khi hệ số học learning cố định thường xảy ra hiện tượng thực hiện bước nhảy gradient lớn, khó khăn trong việc \n","\n"]},{"metadata":{"id":"3sU8bL_h2r6L","colab_type":"text"},"cell_type":"markdown","source":["###3 KẾT QUẢ TRAINING VÀ SO SÁNH VỚI THƯ VIỆN TENSORFLOW\n","\n","- Với cùng một mô hình mạng đơn giản gồm 1 lớp convolution 8 filter (5, 5), 1 lớp drop out ,1 lớp mạng tuyến tính và các siêu tham số learning rate, decay, espoch, batch size giống nhau, Netword tự tạo cho kết quả trainning khá tốt, độ chính xác khoảng 81% với tập test, xấp xỉ với tensorflow khoảng 85% với tập test, nhưng thời gian trainning là rất lâu so với tensorflow.\n","\n","\n","\n","\n","\n","- Với mô hình mạng phức tạp hơn thì mất quá nhiều thời gian tranning , trong khi đó thư viện tensorflow dễ dàng đạt độ chính xác 98%.   \n","\n","\n","\n"]},{"metadata":{"id":"KmB13UEBAHJx","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","###4 HẠN CHẾ CỦA MÔ HÌNH\n","\n","```\n","# This is formatted as code\n","```\n","\n"]},{"metadata":{"id":"Fbe8yIiGFDO7","colab_type":"text"},"cell_type":"markdown","source":["###5 SOURCE CODE\n"]},{"metadata":{"id":"yZDhEutR9aEl","colab_type":"text"},"cell_type":"markdown","source":["#### Import module\n","\n"]},{"metadata":{"id":"kdU7yokAnD6_","colab_type":"code","colab":{}},"cell_type":"code","source":["# import\n","import numpy as np\n","import math\n","import copy\n","import random\n","#from google.colab import drive\n","#drive.mount('/content/gdrive')\n","#import pickle\n","import tensorflow as tf\n","\n","np.random.seed(8)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KlR1GoEc9wif","colab_type":"text"},"cell_type":"markdown","source":["#### Utils functions\n","*create*_x_conv, create_w_conv, read_data \n"]},{"metadata":{"id":"CbzaNEC_sj_R","colab_type":"code","colab":{}},"cell_type":"code","source":["# function create matrix x convolution\n","def create_x_conv(X, window_shape, stride = 1):\n","    channels, rows, cols = X.shape\n","    \n","    vert_blocks = rows - window_shape[0] + 1\n","    horz_blocks = cols - window_shape[1] + 1\n","    \n","    output_vectors = np.zeros((window_shape[0] * window_shape[1] * channels,int((horz_blocks * vert_blocks) / (stride ** 2))))\n","    itr = 0 #count of window \n","    for v_b in range(0,vert_blocks, stride):\n","      for h_b in range(0,horz_blocks, stride):\n","        output_vectors[:, itr] = X[:,v_b : v_b + window_shape[0], h_b: h_b + window_shape[1]].reshape(channels,-1)[:,::-1].ravel()\n","        itr += 1\n","    return output_vectors"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V2t9tivu-__D","colab_type":"code","colab":{}},"cell_type":"code","source":["# function create\n","def create_w_conv(filter, x_shape):\n","  w = np.zeros(filter.shape)\n","  for itr in range(len(w)) :\n","    w[itr] = np.rot90(filter[itr],2)\n","    \n","  num_x,channels, x_rows, x_cols = x_shape\n","  channels, w_rows, w_cols = w.shape\n","  \n","  vert_blocks = x_rows - w_rows + 1\n","  horz_blocks = x_cols - w_cols + 1\n","  result = np.zeros((vert_blocks * horz_blocks,channels * x_rows * x_cols))\n","  \n","  itr = 0\n","  for vb in range(vert_blocks):\n","    for hb in range(horz_blocks):\n","      x_temp = np.zeros(shape = (channels,x_rows,x_cols))\n","      x_temp[:,vb: vb + w_rows,hb : hb + w_cols ] = w\n","      result[itr,:] = x_temp.flatten()\n","      itr += 1\n","      \n","  return result"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ka_xoJhq_EI3","colab_type":"code","colab":{}},"cell_type":"code","source":["# read data mnist\n","def read_data():\n","  mnist = tf.keras.datasets.mnist\n","  (x_train, y_train),(x_test, y_test) = mnist.load_data()\n","  x_train, x_test = x_train / 255.0, x_test / 255.0 \n","  return x_train.reshape(-1,1,28,28), x_test.reshape(-1,1,28,28), y_train, y_test"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HlacuhKK-wbx","colab_type":"text"},"cell_type":"markdown","source":["#### Layers and activations "]},{"metadata":{"id":"ANs1Ac3GFOZG","colab_type":"code","colab":{}},"cell_type":"code","source":["# class parent layer\n","class Layer:\n","    X = None\n","    y = None\n","    denta_x = None\n","    input_shape = ()\n","    output_shape = ()\n","    \n","    def set_x(self, X):\n","        self.X = X\n","        \n","    def init_para(self, input_shape):\n","        pass\n","      \n","    def forward(self):\n","        pass\n","      \n","    def backward(self, denta_y, eta):\n","        pass"],"execution_count":0,"outputs":[]},{"metadata":{"id":"THUEv4oVF90K","colab_type":"code","colab":{}},"cell_type":"code","source":["#class Linear layer\n","class Linear(Layer):\n","    w = []\n","    b = [] \n","    input_shape_old = ()\n","    def __init__(self, num_node):\n","        super(Linear, self).__init__()\n","        self.output_shape = (num_node,) \n","\n","    def init_para(self, input_shape_old):\n","        self.input_shape_old = input_shape_old\n","        input_shape = input_shape_old\n","        if len(input_shape) != 2 :\n","            s = 1\n","            for shape in input_shape:\n","              s = s*shape\n","            input_shape = (s,)\n","            \n","        self.w = np.random.rand(*input_shape , *self.output_shape)\n","        self.b = np.random.rand(1, *self.output_shape)\n","        self.input_shape = input_shape\n","\n","    def forward(self):\n","        if len(self.X.shape) != 2 :\n","            self.X = self.X.reshape(self.X.shape[0],-1)\n","        self.y = np.dot(self.X, self.w) + self.b\n","        return self.y\n","\n","    def backward(self,denta_y, eta):\n","        denta_x =  np.dot(denta_y, self.w.T)\n","        denta_w =  np.dot(self.X.T, denta_y)\n","        denta_b = denta_y.sum(axis = 0)\n","        \n","        #update\n","        self.w =  self.w -eta * denta_w\n","        self.b =  self.b -eta * denta_b\n","        \n","        return denta_x.reshape((-1,) + self.input_shape_old)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"140CLR5RHCsB","colab_type":"code","colab":{}},"cell_type":"code","source":["# class Relu\n","class Relu(Layer):\n","    def forward(self):\n","        self.y = np.maximum(self.X , 0)\n","        return self.y\n","      \n","    def init_para(self, input_shape):\n","        self.input_shape = input_shape\n","        self.output_shape = input_shape\n","        \n","    def backward(self, denta_y, eta):\n","        m = np.ones(self.X.shape)\n","        m[self.X < 0] = 0\n","        self.denta_x = denta_y * m\n","        return self.denta_x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MgMUa4xOy0PQ","colab_type":"code","colab":{}},"cell_type":"code","source":["# class Softmax\n","class Softmax(Layer):\n","    def forward(self):\n","        e_Z = np.exp(self.X - np.max(self.X, axis = 1, keepdims = True))\n","        self.y = e_Z / e_Z.sum(axis = 1, keepdims = True)\n","        return self.y\n","      \n","    def init_para(self, input_shape):\n","        self.input_shape = input_shape \n","        self.output_shape = input_shape \n","        \n","    def backward(self, denta_y, eta):\n","        return denta_y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kI78owBohT-p","colab_type":"code","colab":{}},"cell_type":"code","source":["# class DropOut\n","  class DropOut(Layer):\n","      def __init__(self,dropout,training=True):\n","          self.dropout = dropout\n","          self.training = training\n","          \n","      def init_para(self, input_shape):\n","          self.input_shape = input_shape \n","          self.output_shape = input_shape\n","          \n","      def forward(self):\n","          if self.training:\n","              mP = np.random.binomial(1, self.dropout, size=self.X.shape)\n","          else:\n","              mP = self.dropout\n","          self.y=self.X*mP\n","          return self.y\n","        \n","      def backward(self, denta_y, eta):\n","          self.denta_x = denta_y\n","          return self.denta_x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4SCVD4uEzKxY","colab_type":"code","colab":{}},"cell_type":"code","source":["# class 2d Convolution layer\n","class Convolution(Layer):\n","    filters = None\n","    padding = False\n","    filter_shape = (0,0)\n","    num_filter = 0\n","    X_conv = None\n","    w_conv = None\n","    denta_w = None\n","    \n","    def __init__(self, num_filter, filter_shape):\n","        super(Convolution, self).__init__()\n","        self.filter_shape = filter_shape\n","        self.num_filter = num_filter\n","        \n","    def init_para(self,input_shape):\n","        # get input shape\n","        if len(input_shape) == 2 :\n","          rows, cols = input_shape\n","          input_channels = 1\n","        else :\n","          input_channels, rows, cols = input_shape\n","        \n","        #shape = num filter * input_channels * height * width\n","        self.filters = np.random.rand(self.num_filter, input_channels, self.filter_shape[0], self.filter_shape[1])\n","        \n","        y_rows = input_shape[1] - self.filter_shape[0] + 1\n","        y_cols = input_shape[2] - self.filter_shape[1] + 1\n","        \n","        self.output_shape = (self.num_filter, y_rows, y_cols )\n","        \n","    def create_w_conv(self, y_rows, y_cols ):\n","        \n","        self.w_conv = np.zeros( (self.num_filter,y_rows*y_cols, self.X[0].size) )\n","        for itr in range(self.num_filter):\n","          #w_flatten[itr] = self.filters[itr].flatten()  \n","          self.w_conv[itr] = create_w_conv( self.filters[itr], self.X.shape )\n","          \n","    def create_x_conv(self): \n","        num_window = (self.X.shape[2] - self.filter_shape[0] + 1) * (self.X.shape[3] - self.filter_shape[1] + 1)\n","        self.x_conv = np.zeros( (self.X.shape[0], self.filter_shape[0] * self.filter_shape[1] * self.X.shape[1]  , num_window)  )\n","        for itr in range(self.X.shape[0]):\n","          self.x_conv[itr] = create_x_conv(self.X[itr], self.filter_shape )\n","          \n","    def forward(self):        \n","        y_rows = self.X.shape[2] - self.filter_shape[0] + 1\n","        y_cols = self.X.shape[3] - self.filter_shape[1] + 1\n","        result = np.zeros((self.X.shape[0] ,self.num_filter,y_rows ,y_cols))\n","        num_x = self.X.shape[0]\n","        \n","        x_flat = self.X.reshape((num_x,-1,))\n","        \n","        # create w_conv\n","        self.create_w_conv(y_rows, y_cols)\n","                \n","        #dot matrix\n","        for itr in range(num_x):\n","          for jtr in range(self.num_filter):\n","            result[itr,jtr, : , :] = np.dot(self.w_conv[jtr],x_flat[itr]).reshape(y_rows,y_cols)\n","            \n","        return result\n","      \n","    def backward(self, denta_y, eta):\n","        self.denta_x = np.zeros((self.X.shape))\n","        self.denta_w = np.zeros((self.filters.shape))\n","        y_rows = self.X.shape[2] - self.filter_shape[0] + 1\n","        y_cols = self.X.shape[3] - self.filter_shape[1] + 1\n","        result = np.zeros((self.X.shape[0] ,self.num_filter,y_rows ,y_cols))\n","        num_x, x_channels , x_rows, x_cols = self.X.shape\n","\n","        #create x _ conv\n","        self.create_x_conv()\n","\n","        denta_y_flat = denta_y.reshape((num_x, self.num_filter,-1,))\n","\n","        # denta w\n","        self.denta_w = np.zeros((self.filters.shape))\n","        for jtr in range(self.num_filter):\n","          for itr in range(num_x):\n","            self.denta_w[jtr] += np.dot(denta_y_flat[itr,jtr],self.x_conv[itr].T).reshape(self.filters.shape[1:])\n","          self.denta_w[jtr] = self.denta_w[jtr]/num_x\n","\n","        # denta x  \n","        self.denta_x = np.zeros((self.X.shape))  \n","        for itr in range(num_x):\n","          for jtr in range(self.num_filter):\n","            self.denta_x[itr] += np.dot(self.w_conv[jtr].T, denta_y_flat[itr, jtr]).reshape(self.X.shape[1:])\n","          self.denta_x[itr] = self.denta_x[itr]/self.num_filter\n","\n","        #update\n","        self.filters = self.filters -eta * self.denta_w\n","\n","        return self.denta_x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D9sNOmj1m2OJ","colab_type":"code","cellView":"both","colab":{}},"cell_type":"code","source":["# class MaxPooling\n","class MaxPooling(Layer):\n","  filter_shape = None\n","  stride = 1\n","  def __init__(self, filter_shape ,stride = 1):\n","      self.filter_shape = filter_shape\n","      self.stride = stride\n","        \n","  def init_para(self, input_shape):\n","      self.input_shape = input_shape\n","    \n","      y_rows = input_shape[1] - self.filter_shape[0] + 1\n","      y_cols = input_shape[2] - self.filter_shape[1] + 1\n","        \n","      self.output_shape = (self.input_shape[0], y_rows, y_cols )\n","  def forward(self):\n","      assert len(self.X.shape) == 4\n","      num_x = self.X.shape[0]\n","      channels = self.X.shape[1]\n","      num_window = (self.X.shape[2] - self.filter_shape[0] + 1) * (self.X.shape[3] - self.filter_shape[1] + 1)\n","      self.x_conv = np.zeros( (self.X.shape[0], self.X.shape[1], self.filter_shape[0] * self.filter_shape[1] , num_window)  )\n","      self.max_index = np.zeros((self.X.shape[0], self.X.shape[1],num_window))\n","      \n","      \n","      for itr in range(num_x):\n","        for jtr in range(channels):\n","          self.x_conv[itr, jtr] = create_x_conv(self.X[itr,jtr].reshape(1,self.X.shape[2],self.X.shape[3]), self.filter_shape, self.stride)\n","          self.max_index[itr, jtr] = np.argmax(self.x_conv[itr,jtr], axis=0)\n","      \n","      output = np.zeros((num_x,) + self.output_shape)\n","      for itr in range(num_x):\n","        for jtr in range(channels):\n","            output[itr,jtr] = self.x_conv[itr,jtr, self.max_index[itr,jtr].astype(np.int32) ,np.arange(num_window)].reshape(output.shape[2:])\n","      \n","      return output\n","    \n","  def backward(self,dy):\n","      num_x = self.X.shape[0]\n","      channels = self.X.shape[1]\n","\n","      self.denta_x = np.zeros((self.X.shape))\n","      row_block = self.X.shape[2]-self.filter_shape[0] + 1\n","      col_block = self.X.shape[3]-self.filter_shape[1] + 1\n","\n","      window_size = self.filter_shape[0] *self.filter_shape[1]\n","      re_max_index = abs(self.max_index - window_size) - 1;\n","      for itr in range(num_x):\n","        for jtr in range(channels):\n","            window = 0\n","            row_index = (re_max_index[itr,jtr] / self.filter_shape[0]).astype(np.int32)\n","            col_index = (re_max_index[itr,jtr] - row_index*self.filter_shape[0]).astype(np.int32)\n","            for ktr in range(row_block):\n","              for htr in range(col_block):\n","                  self.denta_x[itr,jtr, ktr + row_index[window], htr + col_index[window]] = d_y[itr,jtr,ktr,htr]\n","                  window +=1\n","      return self.denta_x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Owr3d0b3E3M5","colab_type":"text"},"cell_type":"markdown","source":["#### Network ANN"]},{"metadata":{"id":"_kJR1cOMHMjx","colab_type":"code","colab":{}},"cell_type":"code","source":["# class ANN\n","\n","class ANN:\n","    list_layer = []\n","    best_acc = None\n","    def config_layers(self, x_shape):\n","        self.list_layer[0].init_para(x_shape)\n","        for i in range(1,len(self.list_layer)) :\n","            self.list_layer[i].init_para(self.list_layer[i - 1].output_shape)\n","            \n","    def add(self, layer):\n","        self.list_layer.append(layer)\n","        \n","    def loss(self, y, t):\n","        N = len(y)\n","        return ((t - y)**2).mean(axis = 0)/2\n","\n","    def grad_loss(self,y, t):\n","        return (y - t)/y.shape[0]\n","\n","    def gd(self,y ,t , eta):\n","        denta_y = self.grad_entropy(y, t)\n","        n = len(self.list_layer)\n","        for i in range(n-1,-1,-1):\n","            denta_y = self.list_layer[i].backward(denta_y, eta)\n","    def predict(self,X):\n","        inp = X\n","        for layer in self.list_layer:\n","            layer.X = inp\n","            if(layer is DropOut):\n","                layer.training=False\n","                inp = layer.forward()\n","                layer.training=True\n","            else :\n","                inp = layer.forward()\n","        return inp  \n","      \n","    def entropy_2(self,y, t):\n","        #print(y[:10].argmax(axis = 0))\n","        #print(t[:10])\n","        id0 = range(y.shape[0])\n","        return -np.mean(np.log(y[id0, t]))\n","      \n","    def grad_entropy(self, y, t):\n","        temp_y = copy.copy(y)\n","        id0 = y.shape[0]\n","        temp_y[range(id0),t] -= 1\n","        return temp_y/id0\n","      \n","    def forward(self,X):\n","        inp = X\n","        for layer in self.list_layer:\n","            #print(inp.shape)\n","            layer.X = inp\n","            inp = layer.forward()\n","        return inp\n","    \n","    def accurary(self,y,t):\n","        temp = y.argmax(axis = 1)\n","        n = len(temp[temp == t])\n","        return n * 100 / len(y)\n","    def fit(self, numEpoches, batch, X_train, X_test, t_train , t_test, init_eta,decay_eta,re_train = False,\n","            step = 1):\n","        # set number node for layers \n","        if re_train == False :\n","          self.config_layers(X_train.shape[1:])\n","        num_X = X_train.shape[0]\n","        best_model_acc = None\n","        count_update = 0\n","        eta = init_eta\n","        #train\n","        for i in range(numEpoches):\n","            print('Epoch %d/%d :'%(i+1, numEpoches))\n","            id0 = 0\n","            p = np.random.permutation(len(X_train))\n","            X_train = X_train[p]\n","            t_train = t_train[p]\n","            while id0 < num_X:\n","              #forward\n","              id1 = min(id0 + batch , num_X)\n","              y_train  = self.forward(X_train[id0 : id1])\n","              # gradient decent\n","              self.gd(y_train, t_train[id0 : id1], eta)\n","              if i % step == 0 :\n","                  y_test = self.predict(X_test)\n","                  # print loss and save best model\n","                  loss = self.entropy_2(y_test, t_test)\n","                  acc = self.accurary(y_test, t_test)\n","                  if  (self.best_acc is None) or (acc > self.best_acc):\n","                      self.best_acc = acc\n","                      del best_model_acc\n","                      best_model_acc = copy.deepcopy(self) \n","                      best_model_acc.list_layer = copy.deepcopy(self.list_layer)\n","                      #file2 = open('gdrive/My Drive/model_acc1.obj', 'wb')\n","                      #pickle.dump(best_model_acc, file2)\n","                      count_update += 1\n","                      eta = init_eta * np.exp(-decay_eta*count_update)\n","                  print('    Batch [%d-%d] loss : %f - accurary : %f '%(id0,id1 - 1,loss.sum(axis = 0), acc))\n","              id0 += batch\n","        return best_model_acc"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zufr1U8bFUYR","colab_type":"text"},"cell_type":"markdown","source":["####Compare vs tensorflow\n"]},{"metadata":{"id":"FdXgc3iMjzqg","colab_type":"code","colab":{}},"cell_type":"code","source":["#main program\n","def main(eta,decay_eta, iter,batch, re_train = False):\n","    if re_train == False : \n","        model = ANN()\n","        model.add(Convolution(8, (5,5)))\n","        model.add(Relu())\n","        model.add(DropOut(0.2))\n","        model.add(Linear(num_node = 10))\n","        model.add(Softmax())\n","    else :\n","      f = open('gdrive/My Drive/model_acc1.obj', 'rb')\n","      model = pickle.load(f)\n","      print(model.best_acc)\n","    \n","    x_train, x_test, t_train, t_test = read_data()\n","    best = model.fit(iter,batch, x_train, x_test, t_train, t_test, eta,decay_eta,re_train)\n","    print('final accurary : %f'  %(best.best_acc))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D4eycl4Nw1gi","colab_type":"code","colab":{}},"cell_type":"code","source":["#tensorflow\n","def main_tensor(eta,decay_eta,iter,batch):\n","    x_train, x_test, t_train, t_test = read_data()\n","    model = tf.keras.models.Sequential([\n","      tf.keras.layers.Conv2D( 8,(5,5),activation=tf.nn.relu,data_format='channels_first',input_shape=(1,28,28)),\n","      tf.keras.layers.Dropout(0.2),\n","      tf.keras.layers.Flatten(),\n","      tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n","    ])\n","    adam =tf.keras.optimizers.Adam(lr=eta, decay=decay_eta)\n","    model.compile(optimizer= adam,\n","                  loss='sparse_categorical_crossentropy',\n","                  metrics=['accuracy'])\n","    \n","    model.fit(x_train, t_train,batch_size = batch, epochs=iter, shuffle = True)\n","    \n","    print('final accurary : %f'  %( model.evaluate(x_test, t_test)[1] ))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"raiWo8ChpT2t","colab_type":"code","outputId":"7bdbdbe1-edfd-4e57-83d1-e1bacb3156c7","executionInfo":{"status":"ok","timestamp":1544546043284,"user_tz":-420,"elapsed":96600,"user":{"displayName":"TOÀN NGUYỄN DUY VIỆT","photoUrl":"","userId":"18301338610348705581"}},"colab":{"base_uri":"https://localhost:8080/","height":3679}},"cell_type":"code","source":["#run\n","eta = 1\n","decay_eta = 0.1\n","iter = 100\n","re_train = False # can True if connect in my drive\n","batch = 2000\n","\n","main_tensor(eta,decay_eta,iter,batch)\n","\n","print('=========================================================')\n","print('OUR MODULE :')\n","\n","#main(eta,decay_eta,iter,batch, re_train)\n","\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","Epoch 1/100\n","60000/60000 [==============================] - 5s 92us/step - loss: 7.0007 - acc: 0.5410\n","Epoch 2/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 5.9376 - acc: 0.6314\n","Epoch 3/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 5.5473 - acc: 0.6556\n","Epoch 4/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 5.5043 - acc: 0.6583\n","Epoch 5/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 5.3779 - acc: 0.6661\n","Epoch 6/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 5.1746 - acc: 0.6787\n","Epoch 7/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 5.0081 - acc: 0.6891\n","Epoch 8/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.8730 - acc: 0.6974\n","Epoch 9/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.7501 - acc: 0.7051\n","Epoch 10/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.7049 - acc: 0.7079\n","Epoch 11/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.6478 - acc: 0.7115\n","Epoch 12/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.6245 - acc: 0.7129\n","Epoch 13/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.5999 - acc: 0.7143\n","Epoch 14/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.5759 - acc: 0.7160\n","Epoch 15/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.5617 - acc: 0.7168\n","Epoch 16/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.5235 - acc: 0.7191\n","Epoch 17/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.5168 - acc: 0.7196\n","Epoch 18/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.4679 - acc: 0.7226\n","Epoch 19/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.4569 - acc: 0.7233\n","Epoch 20/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.4454 - acc: 0.7240\n","Epoch 21/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.4049 - acc: 0.7265\n","Epoch 22/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.3800 - acc: 0.7281\n","Epoch 23/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.3801 - acc: 0.7281\n","Epoch 24/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.3616 - acc: 0.7291\n","Epoch 25/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.3599 - acc: 0.7293\n","Epoch 26/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.3713 - acc: 0.7286\n","Epoch 27/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.3441 - acc: 0.7303\n","Epoch 28/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.3350 - acc: 0.7309\n","Epoch 29/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.3328 - acc: 0.7310\n","Epoch 30/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.3223 - acc: 0.7316\n","Epoch 31/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.3093 - acc: 0.7324\n","Epoch 32/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.3029 - acc: 0.7328\n","Epoch 33/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.3185 - acc: 0.7319\n","Epoch 34/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.2973 - acc: 0.7332\n","Epoch 35/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.2984 - acc: 0.7331\n","Epoch 36/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.2831 - acc: 0.7341\n","Epoch 37/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.2849 - acc: 0.7340\n","Epoch 38/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.2663 - acc: 0.7351\n","Epoch 39/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.2610 - acc: 0.7355\n","Epoch 40/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.2517 - acc: 0.7360\n","Epoch 41/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.2369 - acc: 0.7370\n","Epoch 42/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.2379 - acc: 0.7369\n","Epoch 43/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.2503 - acc: 0.7360\n","Epoch 44/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.2400 - acc: 0.7367\n","Epoch 45/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.2220 - acc: 0.7379\n","Epoch 46/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.2126 - acc: 0.7385\n","Epoch 47/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.2154 - acc: 0.7382\n","Epoch 48/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.2117 - acc: 0.7385\n","Epoch 49/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.2013 - acc: 0.7391\n","Epoch 50/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.2174 - acc: 0.7382\n","Epoch 51/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.2100 - acc: 0.7386\n","Epoch 52/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.2148 - acc: 0.7383\n","Epoch 53/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.2126 - acc: 0.7384\n","Epoch 54/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1992 - acc: 0.7393\n","Epoch 55/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1847 - acc: 0.7402\n","Epoch 56/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.1855 - acc: 0.7402\n","Epoch 57/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1884 - acc: 0.7400\n","Epoch 58/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1769 - acc: 0.7407\n","Epoch 59/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1646 - acc: 0.7414\n","Epoch 60/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.1668 - acc: 0.7413\n","Epoch 61/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.1543 - acc: 0.7421\n","Epoch 62/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1685 - acc: 0.7412\n","Epoch 63/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.1599 - acc: 0.7417\n","Epoch 64/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1585 - acc: 0.7418\n","Epoch 65/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1595 - acc: 0.7417\n","Epoch 66/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1529 - acc: 0.7422\n","Epoch 67/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.1693 - acc: 0.7411\n","Epoch 68/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1505 - acc: 0.7423\n","Epoch 69/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1545 - acc: 0.7421\n","Epoch 70/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1571 - acc: 0.7419\n","Epoch 71/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1466 - acc: 0.7426\n","Epoch 72/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1408 - acc: 0.7430\n","Epoch 73/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1393 - acc: 0.7430\n","Epoch 74/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1418 - acc: 0.7429\n","Epoch 75/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.1404 - acc: 0.7429\n","Epoch 76/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.1340 - acc: 0.7433\n","Epoch 77/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1290 - acc: 0.7436\n","Epoch 78/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1249 - acc: 0.7440\n","Epoch 79/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.1280 - acc: 0.7437\n","Epoch 80/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1350 - acc: 0.7432\n","Epoch 81/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1251 - acc: 0.7438\n","Epoch 82/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.1258 - acc: 0.7438\n","Epoch 83/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.1265 - acc: 0.7438\n","Epoch 84/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.1173 - acc: 0.7444\n","Epoch 85/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1073 - acc: 0.7449\n","Epoch 86/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.1074 - acc: 0.7450\n","Epoch 87/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.1048 - acc: 0.7451\n","Epoch 88/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1071 - acc: 0.7449\n","Epoch 89/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1063 - acc: 0.7451\n","Epoch 90/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.0975 - acc: 0.7456\n","Epoch 91/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.1131 - acc: 0.7447\n","Epoch 92/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.1012 - acc: 0.7454\n","Epoch 93/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.0973 - acc: 0.7456\n","Epoch 94/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.0936 - acc: 0.7458\n","Epoch 95/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.0998 - acc: 0.7454\n","Epoch 96/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.0935 - acc: 0.7459\n","Epoch 97/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.0880 - acc: 0.7462\n","Epoch 98/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.0932 - acc: 0.7458\n","Epoch 99/100\n","60000/60000 [==============================] - 1s 15us/step - loss: 4.0881 - acc: 0.7460\n","Epoch 100/100\n","60000/60000 [==============================] - 1s 14us/step - loss: 4.0884 - acc: 0.7462\n","10000/10000 [==============================] - 1s 74us/step\n","final accurary : 0.747000\n","=========================================================\n","OUR MODULE :\n"],"name":"stdout"}]}]}